[training]
    num_epochs = 100
    optimizer = "ADAM"
    optimizer_params = {lr = 0.01}
    loss = "logitcrossentropy"
    loss_params = {dims = 1, agg = "mean"}
    is_supervised = false
    batch_size = 32

[architecture]
    name = "vae_test"
    input_size = [100, 1]
    latent_size = 2
    
submodules = [
    {name = "encoder", path= "./static/vae/encoder.toml"},
    {name = "latent", path="./static/vae/latent.toml"},
    {name = "decoder", path="./static/vae/decoder.toml"},
]
